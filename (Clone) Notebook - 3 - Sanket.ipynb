
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd1e40f5-fd28-4744-83be-7f4366d5ba7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Step 1: Load the Datasets with Schema\n",
    "1. customer-orders.csv\n",
    "- customerid: Integer\n",
    "- productid: Integer\n",
    "- amount: Float\n",
    "2. fakefriends.csv\n",
    "- userid: Integer\n",
    "- name: String\n",
    "- age: Integer\n",
    "- friends: Integer\n",
    "###Step 2: Perform DataFrame Operations\n",
    "- Adding a Column: Add a column to customer_df indicating whether the purchase amount is “High” or “Low”.\n",
    "- Renaming Columns: Rename userid to user_id and friends to num_friends in friends_df.\n",
    "- Removing Columns: Remove the productid column from customer_df.\n",
    "- Filtering Rows: Filter rows where amount > 10 in customer_df.\n",
    "- Joining Multiple DataFrames: Join customer_df with friends_df on customerid and user_id.\n",
    "###Step 3: Perform Aggregation Operations\n",
    "- Count and Count Distinct: Count the total number of rows and distinct customers in customer_df.\n",
    "- Max: Find the maximum purchase amount.\n",
    "- Sum and Sum Distinct: Calculate the total purchase amount and the sum of distinct amounts.\n",
    "- Average and Mean: Calculate the average and mean purchase amount.\n",
    "###Step 4: Grouping Data\n",
    "- Group by Age: Group friends_df by age and calculate: Total number of friends, Average number of friends and Maximum number of friends.\n",
    "- Group by Purchase Category: Group customer_df by purchase_category and calculate: Total amount, Average amount and Count of transactions.\n",
    "###Step 5: Save the Results:\n",
    "- Save the grouped purchase data as a Parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68af6a0f-c365-4a56-8291-000a4fa038c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Define schema for customer-orders.csv\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customerid\", IntegerType(), True),\n",
    "    StructField(\"productid\", IntegerType(), True),\n",
    "    StructField(\"amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Load customer-orders.csv with schema and header=False\n",
    "customer_orders_path = \"file:/Workspace/Users/sanketthodge@gmail.com/customer-orders.csv\"\n",
    "customer_df = spark.read.csv(customer_orders_path, schema=customer_schema, header=False)\n",
    "\n",
    "# Define schema for fakefriends.csv\n",
    "friends_schema = StructType([\n",
    "    StructField(\"userid\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"friends\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load fakefriends.csv with schema and header=False\n",
    "fakefriends_path = \"file:/Workspace/Users/sanketthodge@gmail.com/fakefriends.csv\"\n",
    "friends_df = spark.read.csv(fakefriends_path, schema=friends_schema, header=False)\n",
    "\n",
    "# Display both DataFrames\n",
    "customer_df.show(5)\n",
    "friends_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15dd573-8b9a-42b6-82ba-7d1b585e950c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add a column to customer_df indicating whether the purchase amount is “High” or “Low”\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "customer_df = customer_df.withColumn(\n",
    "    \"purchase_category\",\n",
    "    when(customer_df[\"amount\"] > 50, \"High\").otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "customer_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727e2d58-0434-496c-b292-fc4121ef3aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Rename userid to user_id and friends to num_friends in friends_df.\n",
    "friends_df = friends_df.withColumnRenamed(\"userid\", \"user_id\").withColumnRenamed(\"friends\", \"num_friends\")\n",
    "\n",
    "friends_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc46100f-dd36-4667-b027-2c379139efb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Remove the productid column from customer_df.\n",
    "customer_df = customer_df.drop(\"productid\")\n",
    "\n",
    "customer_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7e23a7-5edb-4aa3-876b-b80f35460ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filter rows where amount > 10 in customer_df.\n",
    "\n",
    "filtered_customer_df = customer_df.filter(customer_df[\"amount\"] > 10)\n",
    "filtered_customer_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f57c77e-303b-4737-9b66-794b1b1f4e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Join customer_df with friends_df on customerid and user_id.\n",
    "\n",
    "# Perform an inner join\n",
    "joined_df = customer_df.join(friends_df, customer_df[\"customerid\"] == friends_df[\"user_id\"], \"inner\")\n",
    "joined_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afb3a93-84bc-45ec-97c2-aff29ac49424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Count the total number of rows and distinct customers in customer_df.\n",
    "\n",
    "# Total rows\n",
    "print(f\"Total rows: {customer_df.count()}\")\n",
    "\n",
    "# Distinct customers\n",
    "print(f\"Distinct customers: {customer_df.select('customerid').distinct().count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ce08e7-e367-4a0d-91a0-163e21ea1282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Find the maximum purchase amount.\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "customer_df.select(max(\"amount\").alias(\"max_purchase\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82759b3-fcca-4e6f-9142-506b5a539516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculate the total purchase amount and the sum of distinct amounts.\n",
    "\n",
    "from pyspark.sql.functions import sum, sum_distinct\n",
    "\n",
    "# Total purchase amount\n",
    "customer_df.select(sum(\"amount\").alias(\"total_purchase\")).show()\n",
    "\n",
    "# Sum of distinct purchase amounts\n",
    "customer_df.select(sum_distinct(\"amount\").alias(\"distinct_total_purchase\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3411e059-3792-4fe5-9799-e0cfd3e2ec92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculate the average and mean purchase amount.\n",
    "\n",
    "from pyspark.sql.functions import avg, mean\n",
    "\n",
    "# Average purchase amount\n",
    "customer_df.select(avg(\"amount\").alias(\"average_purchase\")).show()\n",
    "\n",
    "# Mean purchase amount\n",
    "customer_df.select(mean(\"amount\").alias(\"mean_purchase\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da1e560-4a81-4ff9-81ec-231647a90563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Group friends_df by age and calculate:\n",
    "\t•\tTotal number of friends.\n",
    "\t•\tAverage number of friends.\n",
    "\t•\tMaximum number of friends.'''\n",
    "\n",
    "from pyspark.sql.functions import sum, avg, max\n",
    "\n",
    "friends_df.groupBy(\"age\").agg(\n",
    "    sum(\"num_friends\").alias(\"total_friends\"),\n",
    "    avg(\"num_friends\").alias(\"average_friends\"),\n",
    "    max(\"num_friends\").alias(\"max_friends\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad7ff12-87e1-4b45-8667-6c1be3cb7778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Group customer_df by purchase_category and calculate:\n",
    "\t•\tTotal amount.\n",
    "\t•\tAverage amount.\n",
    "\t•\tCount of transactions.'''\n",
    "\n",
    "from pyspark.sql.functions import sum, avg, count\n",
    "\n",
    "customer_df.groupBy(\"purchase_category\").agg(\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"average_amount\"),\n",
    "    count(\"*\").alias(\"transaction_count\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1625f1-0821-42d9-82ca-060f544c3fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save the grouped purchase data as a Parquet file.\n",
    "\n",
    "output_path = \"dbfs:/FileStore/processed_data/grouped_purchase_data\"\n",
    "customer_df.groupBy(\"purchase_category\").agg(\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"average_amount\"),\n",
    "    count(\"*\").alias(\"transaction_count\")\n",
    ").write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e9eb888-45d1-49e1-b5e4-7f6a2b2d0a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls dbfs:/FileStore/processed_data/grouped_purchase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4acc12c-dac3-49c8-9324-8f843e5c73f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8173681368684509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Notebook - 3 - Sanket",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
Request1233
